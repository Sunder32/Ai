import os
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import requests
import json
from typing import List, Optional
import shutil
import subprocess

# –ò–º–ø–æ—Ä—Ç RAG –∏ Fine-tuning –º–æ–¥—É–ª–µ–π
from rag_engine import get_rag_engine
from finetune_prepare import prepare_chat_format, create_ollama_training_modelfile, get_dataset_info
from learning_engine import get_learning_engine

app = FastAPI(title="DeepSeek Mini-Site API")


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OLLAMA_API_URL = "http://localhost:11434/api/generate"
# Allow overriding the model via env var so you can point to a finetuned model
MODEL_NAME = os.getenv("PROJECT_MODEL_NAME", "deepseek-project-model")
UPLOAD_DIR = "uploads"
DATASET_FILE = "dataset.jsonl"

os.makedirs(UPLOAD_DIR, exist_ok=True)

class ChatRequest(BaseModel):
    prompt: str
    context: Optional[List[int]] = []
    use_learning: Optional[bool] = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—É—á–µ–Ω–∏—è

class ChatResponse(BaseModel):
    response: str
    context: List[int]

class CorrectionRequest(BaseModel):
    prompt: str
    original_response: str
    corrected_response: str
    feedback: Optional[str] = ""

class LikeRequest(BaseModel):
    prompt: str
    response: str

@app.post("/api/chat")
async def chat(request: ChatRequest):
    import traceback
    
    try:
        print(f"\n[CHAT] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {request.prompt[:50]}...")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—É—á–µ–Ω–∏—è –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        learning_context = ""
        if request.use_learning:
            learning = get_learning_engine()
            learning_context = learning.get_learning_context(max_examples=5)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è
        enhanced_prompt = request.prompt
        if learning_context:
            enhanced_prompt = f"""{learning_context}

---
–¢–µ–ø–µ—Ä—å –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —É—á–∏—Ç—ã–≤–∞—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—ã—à–µ:

{request.prompt}"""
        
        payload = {
            "model": MODEL_NAME,
            "prompt": enhanced_prompt,
            "stream": False,
            "context": request.context if request.context else []
        }
        
        print(f"[CHAT] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Ollama —Å –º–æ–¥–µ–ª—å—é: {MODEL_NAME}")
        
        try:
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)
            print(f"[CHAT] Ollama response status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"[CHAT] Ollama error response: {response.text}")
                raise HTTPException(
                    status_code=response.status_code, 
                    detail=f"Ollama returned error: {response.text}"
                )
            
            response.raise_for_status()
            data = response.json()
            
            print(f"[CHAT] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏: {data.get('response', '')[:50]}...")
            
        except requests.exceptions.Timeout:
            print("[CHAT] ‚ùå Timeout waiting for Ollama response")
            raise HTTPException(status_code=504, detail="Ollama request timed out")
        except requests.exceptions.ConnectionError:
            print("[CHAT] ‚ùå Cannot connect to Ollama service")
            raise HTTPException(status_code=503, detail="Ollama service is not reachable. Is it running?")
        
        log_entry = {
            "prompt": request.prompt,
            "response": data.get("response", ""),
            "model": MODEL_NAME
        }
        
        try:
            with open(DATASET_FILE, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as log_error:
            print(f"[CHAT] Warning: Failed to log conversation: {log_error}")

        return {
            "response": data.get("response", ""),
            "context": data.get("context", [])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        error_details = traceback.format_exc()
        print(f"[CHAT] ‚ùå Unexpected error:\n{error_details}")
        raise HTTPException(status_code=500, detail=f"Chat failed: {str(e)}")

@app.post("/api/train/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        return {"message": f"File '{file.filename}' uploaded successfully."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def list_models():
    try:
        response = requests.get("http://localhost:11434/api/tags")
        response.raise_for_status()
        return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/train/files")
async def get_uploaded_files():
    try:
        files = []
        if os.path.exists(UPLOAD_DIR):
            for filename in os.listdir(UPLOAD_DIR):
                file_path = os.path.join(UPLOAD_DIR, filename)
                if os.path.isfile(file_path):
                    files.append({
                        "name": filename,
                        "size": os.path.getsize(file_path)
                    })
        return {"files": files, "count": len(files)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/train/start")
async def start_training():
    import traceback
    
    try:
        print("\n" + "="*80)
        print("üöÄ  –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ù–ï–ô–†–û–°–ï–¢–ò  üöÄ".center(80))
        print("="*80)
        print(f"[TRAINING] –í—Ä–µ–º—è —Å—Ç–∞—Ä—Ç–∞: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("-"*80 + "\n")
        
        if not os.path.exists(UPLOAD_DIR) or not os.listdir(UPLOAD_DIR):
            print("[TRAINING]  No files found in upload directory")
            raise HTTPException(status_code=400, detail="No files uploaded for training")
        
        all_text = []
        files_in_dir = os.listdir(UPLOAD_DIR)
        print(f"[TRAINING] Found {len(files_in_dir)} files in upload directory")
        
        for filename in files_in_dir:
            file_path = os.path.join(UPLOAD_DIR, filename)
            if not os.path.isfile(file_path):
                continue
                
            file_ext = os.path.splitext(filename)[1].lower()
            print(f"[TRAINING] Processing file: {filename} (type: {file_ext})")
            
            try:
                if file_ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        all_text.append(content)
                        print(f"[TRAINING] Successfully read TXT file: {len(content)} characters")
                
                elif file_ext == '.pdf':
                    try:
                        from pypdf import PdfReader
                        reader = PdfReader(file_path)
                        text = ""
                        for i, page in enumerate(reader.pages):
                            page_text = page.extract_text()
                            text += page_text
                            print(f"[TRAINING] PDF page {i+1}: {len(page_text)} characters")
                        all_text.append(text)
                        print(f"[TRAINING] Successfully read PDF file: {len(text)} characters total")
                    except Exception as pdf_error:
                        print(f"[ERROR] Error processing PDF {filename}: {str(pdf_error)}")
                        traceback.print_exc()
                
                elif file_ext in ['.docx', '.doc']:
                    try:
                        import docx
                        doc = docx.Document(file_path)
                        text = "\n".join([para.text for para in doc.paragraphs])
                        all_text.append(text)
                        print(f"[TRAINING] Successfully read DOCX file: {len(text)} characters")
                    except Exception as docx_error:
                        print(f"[ERROR] Error processing DOCX {filename}: {str(docx_error)}")
                        traceback.print_exc()
                
                elif file_ext == '.json':
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ (–¥–∞—Ç–∞—Å–µ—Ç)
                        if isinstance(data, list):
                            text_parts = []
                            for item in data:
                                if isinstance(item, dict):
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
                                    for key in ['text', 'content', 'input', 'output', 'question', 'answer', 'instruction']:
                                        if key in item and item[key]:
                                            text_parts.append(str(item[key]))
                                else:
                                    text_parts.append(str(item))
                            text = "\n".join(text_parts)
                        else:
                            text = json.dumps(data, ensure_ascii=False, indent=2)
                        
                        if text.strip():
                            all_text.append(text)
                            print(f"[TRAINING] Successfully read JSON file: {len(text)} characters")
                    except Exception as json_error:
                        print(f"[ERROR] Error processing JSON {filename}: {str(json_error)}")
                        traceback.print_exc()
                
                elif file_ext == '.jsonl':
                    try:
                        text_parts = []
                        with open(file_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                if line.strip():
                                    item = json.loads(line)
                                    if isinstance(item, dict):
                                        for key in ['text', 'content', 'input', 'output', 'prompt', 'response']:
                                            if key in item and item[key]:
                                                text_parts.append(str(item[key]))
                        text = "\n".join(text_parts)
                        if text.strip():
                            all_text.append(text)
                            print(f"[TRAINING] Successfully read JSONL file: {len(text)} characters")
                    except Exception as jsonl_error:
                        print(f"[ERROR] Error processing JSONL {filename}: {str(jsonl_error)}")
                        traceback.print_exc()
                        
            except Exception as e:
                print(f"[ERROR] Error processing {filename}: {str(e)}")
                traceback.print_exc()
                continue
        
        if not all_text:
            print("[ERROR] Could not extract text from any files")
            raise HTTPException(status_code=400, detail="Could not extract text from any files")
        
        combined_text = "\n\n".join(all_text)
        print(f"[TRAINING] Combined text length: {len(combined_text)} characters")
        
        training_file = "training_data.txt"
        with open(training_file, 'w', encoding='utf-8') as f:
            f.write(combined_text)
        print(f"[TRAINING] Saved training data to {training_file}")
        
        context_snippet = combined_text[:6000] if len(combined_text) > 6000 else combined_text
        
        # Escape special characters and format context for SYSTEM prompt
        # Replace newlines with spaces and remove problematic characters
        clean_context = context_snippet.replace('\n', ' ').replace('\r', ' ').replace('"', "'")
        # Remove multiple spaces
        import re
        clean_context = re.sub(r'\s+', ' ', clean_context).strip()
        
        modelfile_content = f'''FROM deepseek-r1:8b
PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM """You are DeepSeek, an AI assistant with specialized knowledge from the provided documents. Use this knowledge to provide accurate and helpful responses.

Here is your knowledge base context:

{clean_context}

Based on this context and your general knowledge, respond helpfully and accurately to user questions."""'''

        modelfile_path = os.path.abspath("Modelfile")
        with open(modelfile_path, "w", encoding="utf-8") as f:
            f.write(modelfile_content)
        print(f"[TRAINING] Updated Modelfile at: {modelfile_path}")
        
        # Verify Modelfile exists
        if not os.path.exists(modelfile_path):
            raise HTTPException(status_code=500, detail="Failed to create Modelfile")
        
        # Recreate model with Ollama
        print("[TRAINING] Attempting to delete old model...")
        try:
            delete_response = requests.delete(
                "http://localhost:11434/api/delete", 
                json={"name": MODEL_NAME},
                timeout=10
            )
            print(f"[TRAINING] Delete response: {delete_response.status_code}")
        except Exception as del_error:
            print(f"[TRAINING] Delete failed (this is OK if model doesn't exist): {str(del_error)}")
        
        print("[TRAINING] Creating new model...")
        try:
            # Use subprocess to create model via ollama CLI (more reliable)
            print(f"[TRAINING] Using Modelfile at: {modelfile_path}")
            
            # Run ollama create command
            result = subprocess.run(
                ["ollama", "create", MODEL_NAME, "-f", modelfile_path],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes timeout
            )
            
            print(f"[TRAINING] Ollama create stdout: {result.stdout}")
            if result.stderr:
                print(f"[TRAINING] Ollama create stderr: {result.stderr}")
            
            if result.returncode != 0:
                error_msg = result.stderr or result.stdout or "Unknown error"
                print(f"[ERROR] Failed to create model: {error_msg}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"Failed to create model: {error_msg}"
                )
            
            print(f"[TRAINING] Model '{MODEL_NAME}' created successfully!")
            
        except subprocess.TimeoutExpired:
            print("[ERROR] Timeout while creating model")
            raise HTTPException(status_code=500, detail="Model creation timed out")
        except FileNotFoundError:
            print("[ERROR] Ollama command not found. Make sure Ollama is installed and in PATH")
            raise HTTPException(status_code=503, detail="Ollama is not installed or not in PATH")
        except Exception as create_error:
            print(f"[ERROR] Error creating model: {str(create_error)}")
            raise HTTPException(status_code=500, detail=f"Failed to create model: {str(create_error)}")
        

        print("\n" + "-"*80)
        print(" –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!  ‚úÖ".center(80))
        print("="*80)
        print(f"[TRAINING] –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[TRAINING] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_text)}")
        print(f"[TRAINING] –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(combined_text)}")
        print(f"[TRAINING] –ú–æ–¥–µ–ª—å: {MODEL_NAME}")
        print("="*80 + "\n")
        
        return {
            "message": "Training completed successfully",
            "files_processed": len(all_text),
            "model_name": MODEL_NAME,
            "total_chars": len(combined_text)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        error_details = traceback.format_exc()
        print(f"[ERROR] Training failed with exception:\n{error_details}")
        raise HTTPException(status_code=500, detail=f"Training failed: {str(e)}")


# ============== RAG API ==============

class RAGSearchRequest(BaseModel):
    query: str
    n_results: int = 5

@app.post("/api/rag/index")
async def rag_index_documents():
    """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ RAG (–≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î)"""
    import traceback
    
    try:
        print("\n[RAG] –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        if not os.path.exists(UPLOAD_DIR) or not os.listdir(UPLOAD_DIR):
            raise HTTPException(status_code=400, detail="–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
        
        rag = get_rag_engine()
        rag.clear()  # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        all_texts = []
        all_metadatas = []
        
        for filename in os.listdir(UPLOAD_DIR):
            file_path = os.path.join(UPLOAD_DIR, filename)
            if not os.path.isfile(file_path):
                continue
            
            file_ext = os.path.splitext(filename)[1].lower()
            text = ""
            
            try:
                if file_ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                
                elif file_ext == '.pdf':
                    from pypdf import PdfReader
                    reader = PdfReader(file_path)
                    text = "\n".join(page.extract_text() for page in reader.pages)
                
                elif file_ext in ['.docx', '.doc']:
                    import docx
                    doc = docx.Document(file_path)
                    text = "\n".join(para.text for para in doc.paragraphs)
                
                elif file_ext == '.json':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list):
                        text = "\n".join(str(item) for item in data)
                    else:
                        text = json.dumps(data, ensure_ascii=False)
                
                if text.strip():
                    all_texts.append(text)
                    all_metadatas.append({"filename": filename})
                    print(f"[RAG] –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {filename} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    
            except Exception as e:
                print(f"[RAG] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
                continue
        
        if not all_texts:
            raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
        chunks_count = rag.add_documents(all_texts, all_metadatas)
        
        print(f"[RAG] ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {chunks_count} —á–∞–Ω–∫–æ–≤")
        
        return {
            "message": "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
            "files_processed": len(all_texts),
            "chunks_created": chunks_count
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[RAG] ‚ùå –û—à–∏–±–∫–∞: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/rag/search")
async def rag_search(request: RAGSearchRequest):
    """–ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ RAG"""
    try:
        rag = get_rag_engine()
        results = rag.search(request.query, request.n_results)
        
        return {
            "query": request.query,
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/rag/stats")
async def rag_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG –±–∞–∑—ã"""
    try:
        rag = get_rag_engine()
        return rag.get_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/chat/rag")
async def chat_with_rag(request: ChatRequest):
    """–ß–∞—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG (–ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º)"""
    import traceback
    
    try:
        print(f"\n[RAG-CHAT] –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {request.prompt[:50]}...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG
        rag = get_rag_engine()
        context = rag.get_context_for_query(request.prompt, max_tokens=2000)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if context:
            enhanced_prompt = f"""–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å:

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: {request.prompt}

–û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º."""
        else:
            enhanced_prompt = request.prompt
        
        payload = {
            "model": MODEL_NAME,
            "prompt": enhanced_prompt,
            "stream": False,
            "context": request.context if request.context else []
        }
        
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)
        response.raise_for_status()
        data = response.json()
        
        return {
            "response": data.get("response", ""),
            "context": data.get("context", []),
            "rag_context_used": bool(context)
        }
        
    except Exception as e:
        print(f"[RAG-CHAT] ‚ùå –û—à–∏–±–∫–∞: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== FINE-TUNING API ==============

@app.post("/api/finetune/prepare")
async def prepare_finetune_data():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è fine-tuning –∏–∑ JSON –∏ TXT —Ñ–∞–π–ª–æ–≤"""
    try:
        print("\n[FINETUNE] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fine-tuning...")
        
        prepared_files = []
        
        for filename in os.listdir(UPLOAD_DIR):
            file_ext = os.path.splitext(filename)[1].lower()
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º JSON –∏ TXT —Ñ–∞–π–ª—ã
            if file_ext not in ['.json', '.txt']:
                continue
                
            file_path = os.path.join(UPLOAD_DIR, filename)
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
                info = get_dataset_info(file_path)
                print(f"[FINETUNE] –î–∞—Ç–∞—Å–µ—Ç {filename}: {info}")
                
                if "error" in info:
                    print(f"[FINETUNE] –ü—Ä–æ–ø—É—Å–∫ {filename}: {info['error']}")
                    continue
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
                if file_ext == '.json':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if not isinstance(data, list):
                        print(f"[FINETUNE] –ü—Ä–æ–ø—É—Å–∫ {filename}: –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º")
                        continue
                    
                    output_path = prepare_chat_format(data, file_path)
                    
                elif file_ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    output_path = prepare_chat_format(text, file_path)
                
                prepared_files.append({
                    "source": filename,
                    "output": os.path.basename(output_path),
                    "info": info
                })
                
                print(f"[FINETUNE] ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {output_path}")
                
            except Exception as e:
                print(f"[FINETUNE] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {filename}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if not prepared_files:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω–æ JSON –∏–ª–∏ TXT —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è fine-tuning")
        
        return {
            "message": "–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è fine-tuning",
            "files": prepared_files
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/finetune/create-model")
async def create_finetuned_model():
    """–°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    try:
        print("\n[FINETUNE] –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏...")
        
        # –ò—â–µ–º JSON –∏–ª–∏ TXT –¥–∞—Ç–∞—Å–µ—Ç
        dataset_path = None
        for filename in os.listdir(UPLOAD_DIR):
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext in ['.json', '.txt']:
                dataset_path = os.path.join(UPLOAD_DIR, filename)
                break
        
        if not dataset_path:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω JSON –∏–ª–∏ TXT –¥–∞—Ç–∞—Å–µ—Ç")
        
        print(f"[FINETUNE] –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
        
        # –°–æ–∑–¥–∞—ë–º Modelfile —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        modelfile_path = os.path.abspath("Modelfile.finetune")
        result_path = create_ollama_training_modelfile(
            base_model="deepseek-r1:8b",
            dataset_path=dataset_path,
            output_path=modelfile_path
        )
        
        print(f"[FINETUNE] –°–æ–∑–¥–∞–Ω Modelfile: {result_path}")
        
        # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å –≤ Ollama
        model_name = "deepseek-finetuned"
        
        result = subprocess.run(
            ["ollama", "create", model_name, "-f", modelfile_path],
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode != 0:
            error_msg = result.stderr or result.stdout or "Unknown error"
            print(f"[FINETUNE] ‚ùå –û—à–∏–±–∫–∞ Ollama: {error_msg}")
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {error_msg}")
        
        print(f"[FINETUNE] ‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å–æ–∑–¥–∞–Ω–∞!")
        
        return {
            "message": f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞",
            "model_name": model_name,
            "modelfile": modelfile_path
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"[FINETUNE] ‚ùå –û—à–∏–±–∫–∞: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/finetune/info")
async def get_finetune_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –¥–ª—è fine-tuning (JSON –∏ TXT)"""
    try:
        datasets = []
        
        if not os.path.exists(UPLOAD_DIR):
            return {"datasets": [], "message": "–ü–∞–ø–∫–∞ uploads –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"}
        
        for filename in os.listdir(UPLOAD_DIR):
            file_ext = os.path.splitext(filename)[1].lower()
            
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º JSON, TXT –∏ JSONL
            if file_ext not in ['.json', '.txt', '.jsonl']:
                continue
                
            file_path = os.path.join(UPLOAD_DIR, filename)
            
            try:
                info = get_dataset_info(file_path)
                info["filename"] = filename
                datasets.append(info)
            except Exception as e:
                print(f"[FINETUNE] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
                datasets.append({
                    "filename": filename,
                    "error": str(e)
                })
        
        return {"datasets": datasets, "count": len(datasets)}
        
    except Exception as e:
        import traceback
        print(f"[FINETUNE] ‚ùå –û—à–∏–±–∫–∞ get_finetune_info: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== LEARNING API (–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ) ==============

@app.post("/api/learning/correct")
async def submit_correction(request: CorrectionRequest):
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –ò–ò
    –ò–ò –∑–∞–ø–æ–º–Ω–∏—Ç –æ—à–∏–±–∫—É –∏ –Ω–µ –±—É–¥–µ—Ç –µ—ë –ø–æ–≤—Ç–æ—Ä—è—Ç—å
    """
    try:
        learning = get_learning_engine()
        result = learning.add_correction(
            prompt=request.prompt,
            original_response=request.original_response,
            corrected_response=request.corrected_response,
            feedback=request.feedback or ""
        )
        
        print(f"[LEARNING] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {request.prompt[:50]}...")
        
        return {
            "message": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ! –ò–ò —É—á—Ç—ë—Ç —ç—Ç–æ –≤ –±—É–¥—É—â–∏—Ö –æ—Ç–≤–µ—Ç–∞—Ö.",
            "correction_id": result["id"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/learning/like")
async def like_response(request: LikeRequest):
    """
    –õ–∞–π–∫–Ω—É—Ç—å —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç –ò–ò
    –ò–ò –±—É–¥–µ—Ç —Å—Ç–∞—Ä–∞—Ç—å—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –ø–æ—Ö–æ–∂–∏–º –æ–±—Ä–∞–∑–æ–º
    """
    try:
        learning = get_learning_engine()
        result = learning.add_good_response(
            prompt=request.prompt,
            response=request.response
        )
        
        print(f"[LEARNING] üëç –û—Ç–≤–µ—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ —Ö–æ—Ä–æ—à–∏–π")
        
        return {
            "message": "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ç–∑—ã–≤! –ò–ò –±—É–¥–µ—Ç —É—á–∏—Ç—å—Å—è –Ω–∞ —Ö–æ—Ä–æ—à–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.",
            "id": result["id"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/learning/stats")
async def get_learning_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        learning = get_learning_engine()
        return learning.get_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/learning/export")
async def export_learning_data():
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è fine-tuning"""
    try:
        learning = get_learning_engine()
        examples = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        for corr in learning.corrections:
            examples.append({
                "messages": [
                    {"role": "user", "content": corr['prompt']},
                    {"role": "assistant", "content": corr['corrected_response']}
                ]
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–≤–µ—Ç—ã
        for good in learning.good_responses:
            examples.append({
                "messages": [
                    {"role": "user", "content": good['prompt']},
                    {"role": "assistant", "content": good['response']}
                ]
            })
        
        return {
            "examples": examples,
            "count": len(examples)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== FILE RESPONSES ==============

@app.post("/api/chat/file")
async def chat_with_file_response(request: ChatRequest):
    """
    –ß–∞—Ç —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ—Ç–≤–µ—Ç–∞ —Ñ–∞–π–ª–æ–º (Markdown, –∫–æ–¥ –∏ —Ç.–ø.)
    """
    import traceback
    import re
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è —Ñ–∞–π–ª–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        enhanced_prompt = f"""–¢—ã –º–æ–∂–µ—à—å –æ—Ç–≤–µ—á–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown. 
–ò—Å–ø–æ–ª—å–∑—É–π:
- –ó–∞–≥–æ–ª–æ–≤–∫–∏ (# ## ###)
- –°–ø–∏—Å–∫–∏ (- –∏–ª–∏ 1.)
- –ö–æ–¥ (```—è–∑—ã–∫ ... ```)
- –¢–∞–±–ª–∏—Ü—ã
- –í—ã–¥–µ–ª–µ–Ω–∏–µ (**–∂–∏—Ä–Ω—ã–π**, *–∫—É—Ä—Å–∏–≤*)

–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª, –∫–æ–¥ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç ‚Äî –æ—Ñ–æ—Ä–º–∏ —ç—Ç–æ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ.

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {request.prompt}"""

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—É—á–µ–Ω–∏—è
        learning = get_learning_engine()
        learning_context = learning.get_learning_context(max_examples=3)
        
        if learning_context:
            enhanced_prompt = f"{learning_context}\n\n---\n\n{enhanced_prompt}"
        
        payload = {
            "model": MODEL_NAME,
            "prompt": enhanced_prompt,
            "stream": False,
            "context": request.context if request.context else []
        }
        
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        data = response.json()
        
        response_text = data.get("response", "")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = "text"
        file_extension = None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –±–ª–æ–∫–∏ –∫–æ–¥–∞
        code_blocks = re.findall(r'```(\w+)?\n(.*?)```', response_text, re.DOTALL)
        
        if code_blocks:
            lang = code_blocks[0][0] or "txt"
            content_type = "code"
            file_extension = {
                "python": "py", "javascript": "js", "typescript": "ts",
                "html": "html", "css": "css", "json": "json",
                "markdown": "md", "md": "md", "sql": "sql",
                "bash": "sh", "shell": "sh", "yaml": "yaml", "yml": "yaml"
            }.get(lang.lower(), lang.lower())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ Markdown –¥–æ–∫—É–º–µ–Ω—Ç
        if re.search(r'^#{1,3}\s', response_text, re.MULTILINE):
            content_type = "markdown"
            file_extension = "md"
        
        return {
            "response": response_text,
            "context": data.get("context", []),
            "content_type": content_type,
            "file_extension": file_extension,
            "can_download": content_type in ["code", "markdown"]
        }
        
    except Exception as e:
        print(f"[CHAT-FILE] ‚ùå –û—à–∏–±–∫–∞: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


app.mount("/", StaticFiles(directory="client", html=True), name="static")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "5050"))
    uvicorn.run(app, host="0.0.0.0", port=port)
